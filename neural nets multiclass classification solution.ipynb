{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ilyas Ustun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Multiclass Classification Exercise Solutions\n",
    "\n",
    "## Objective\n",
    "In this exercise, you will build and train a neural network to classify wine types using the Wine dataset. You'll learn how to:\n",
    "\n",
    "1. Load and explore classification data\n",
    "2. Prepare data for neural network classification\n",
    "3. Build a neural network for multiclass classification\n",
    "4. Train and evaluate the classification model\n",
    "5. Visualize classification results and performance metrics\n",
    "\n",
    "## Dataset\n",
    "We'll use the Wine dataset from sklearn, which contains chemical analysis of wines from three different cultivars. Our goal is to classify wines into one of three classes based on 13 chemical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Import Required Libraries\n",
    "\n",
    "**Solution Explanation:**\n",
    "We import all necessary libraries for data handling, model building, and visualization. Each library serves a specific purpose:\n",
    "- `tensorflow`: For building and training neural networks\n",
    "- `numpy`: For numerical operations\n",
    "- `matplotlib` & `seaborn`: For creating visualizations\n",
    "- `sklearn`: For dataset loading, preprocessing, and evaluation metrics\n",
    "- `pandas`: For data manipulation and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Load and Explore the Dataset\n",
    "\n",
    "**Solution Explanation:**\n",
    "We load the Wine dataset and explore its structure to understand what we're working with. This includes checking the shape of data, feature names, target classes, and their distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Feature names: {wine.feature_names}\")\n",
    "print(f\"Target names: {wine.target_names}\")\n",
    "print(f\"Number of classes: {len(wine.target_names)}\")\n",
    "print(f\"Dataset description: {wine.DESCR[:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for easier exploration\n",
    "df = pd.DataFrame(X, columns=wine.feature_names)\n",
    "df['target'] = y\n",
    "df['target_name'] = [wine.target_names[i] for i in y]\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass Distribution:\")\n",
    "class_counts = pd.Series(y).value_counts().sort_index()\n",
    "for i, (class_idx, count) in enumerate(class_counts.items()):\n",
    "    print(f\"Class {class_idx} ({wine.target_names[class_idx]}): {count} samples ({count/len(y)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution and feature relationships\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Class distribution\n",
    "axes[0, 0].bar(range(len(wine.target_names)), class_counts.values, \n",
    "               color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "axes[0, 0].set_xlabel('Wine Class')\n",
    "axes[0, 0].set_ylabel('Number of Samples')\n",
    "axes[0, 0].set_title('Distribution of Wine Classes')\n",
    "axes[0, 0].set_xticks(range(len(wine.target_names)))\n",
    "axes[0, 0].set_xticklabels(wine.target_names, rotation=45)\n",
    "\n",
    "# Feature correlation heatmap (top 8 features)\n",
    "top_features = ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', \n",
    "                'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols']\n",
    "corr_matrix = df[top_features].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            ax=axes[0, 1], fmt='.2f')\n",
    "axes[0, 1].set_title('Feature Correlation Matrix (Top 8 Features)')\n",
    "\n",
    "# Scatter plot of two important features\n",
    "colors = ['red', 'blue', 'green']\n",
    "for i, target_name in enumerate(wine.target_names):\n",
    "    mask = y == i\n",
    "    axes[1, 0].scatter(X[mask, 0], X[mask, 6], \n",
    "                      c=colors[i], label=target_name, alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Alcohol')\n",
    "axes[1, 0].set_ylabel('Flavanoids')\n",
    "axes[1, 0].set_title('Wine Classes by Alcohol vs Flavanoids')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot of alcohol content by class\n",
    "df.boxplot(column='alcohol', by='target_name', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Alcohol Content Distribution by Wine Class')\n",
    "axes[1, 1].set_xlabel('Wine Class')\n",
    "axes[1, 1].set_ylabel('Alcohol Content')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- Dataset is relatively balanced with slight imbalance\")\n",
    "print(\"- Features show different scales (need normalization)\")\n",
    "print(\"- Clear separation between classes in alcohol vs flavanoids plot\")\n",
    "print(\"- Some features are correlated (multicollinearity present)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Prepare the Data\n",
    "\n",
    "**Solution Explanation:**\n",
    "Data preparation for classification includes:\n",
    "1. Split data into training (70%), validation (15%), and testing (15%) sets\n",
    "2. Normalize features using StandardScaler\n",
    "3. Convert target labels to categorical format (one-hot encoding) for neural networks\n",
    "4. Calculate class weights to handle potential imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.176, random_state=42, stratify=y_temp  # 0.176 * 0.85 â‰ˆ 0.15\n",
    ")\n",
    "\n",
    "print(\"Data split completed:\")\n",
    "print(f\"Training samples: {X_train.shape[0]} ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Validation samples: {X_val.shape[0]} ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Testing samples: {X_test.shape[0]} ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Check class distribution in each set\n",
    "print(\"\\nClass distribution in each set:\")\n",
    "for name, y_subset in [('Train', y_train), ('Validation', y_val), ('Test', y_test)]:\n",
    "    counts = np.bincount(y_subset)\n",
    "    print(f\"{name}: {counts} -> {counts/len(y_subset)*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Data normalization completed:\")\n",
    "print(f\"Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Validation set shape: {X_val_scaled.shape}\")\n",
    "print(f\"Test set shape: {X_test_scaled.shape}\")\n",
    "\n",
    "# Verify normalization\n",
    "print(f\"\\nAfter normalization - Training data mean: {np.mean(X_train_scaled):.6f}\")\n",
    "print(f\"After normalization - Training data std: {np.std(X_train_scaled):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to categorical (one-hot encoding)\n",
    "num_classes = len(wine.target_names)\n",
    "y_train_categorical = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val_categorical = tf.keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test_categorical = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(\"Label encoding completed:\")\n",
    "print(f\"Original labels shape: {y_train.shape}\")\n",
    "print(f\"Categorical labels shape: {y_train_categorical.shape}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Show example of one-hot encoding\n",
    "print(\"\\nExample of one-hot encoding:\")\n",
    "print(f\"Original label: {y_train[0]} ({wine.target_names[y_train[0]]})\")\n",
    "print(f\"One-hot encoded: {y_train_categorical[0]}\")\n",
    "\n",
    "# Calculate class weights for handling imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(f\"\\nClass weights: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Build a Neural Network for Classification\n",
    "\n",
    "**Solution Explanation:**\n",
    "We create a Sequential model with:\n",
    "- Input layer: 13 features (wine chemical properties)\n",
    "- Hidden layer 1: 64 neurons with ReLU activation\n",
    "- Hidden layer 2: 32 neurons with ReLU activation\n",
    "- Hidden layer 3: 16 neurons with ReLU activation\n",
    "- Output layer: 3 neurons with softmax activation for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Sequential model for classification\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')  # Softmax for multiclass\n",
    "])\n",
    "\n",
    "print(\"Model created successfully!\")\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(\"- Input layer: 13 features (wine chemical properties)\")\n",
    "print(\"- Hidden layer 1: 64 neurons with ReLU activation\")\n",
    "print(\"- Hidden layer 2: 32 neurons with ReLU activation\")\n",
    "print(\"- Hidden layer 3: 16 neurons with ReLU activation\")\n",
    "print(\"- Output layer: 3 neurons with softmax activation (multiclass)\")\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Compile the Model\n",
    "\n",
    "**Solution Explanation:**\n",
    "Model compilation configures the training process:\n",
    "- **Optimizer**: Adam (adaptive learning rate, works well for most problems)\n",
    "- **Loss function**: Categorical crossentropy (standard for multiclass classification)\n",
    "- **Metrics**: Accuracy (percentage of correct predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model for training\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',    # For multiclass classification\n",
    "    metrics=['accuracy']                # Classification accuracy\n",
    ")\n",
    "\n",
    "print(\"Model compiled successfully!\")\n",
    "print(\"\\nCompilation settings:\")\n",
    "print(\"- Optimizer: Adam (adaptive learning rate)\")\n",
    "print(\"- Loss function: Categorical Crossentropy\")\n",
    "print(\"- Metrics: Accuracy\")\n",
    "print(\"\\nKey differences from regression:\")\n",
    "print(\"- Loss: MSE â†’ Categorical Crossentropy\")\n",
    "print(\"- Metrics: MAE â†’ Accuracy\")\n",
    "print(\"- Output activation: None â†’ Softmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Train the Model\n",
    "\n",
    "**Solution Explanation:**\n",
    "Training parameters:\n",
    "- **Epochs**: 150 (number of complete passes through the training data)\n",
    "- **Batch size**: 16 (smaller batch size for small dataset)\n",
    "- **Validation data**: Separate validation set for monitoring\n",
    "- **Class weights**: Handle class imbalance\n",
    "- **Verbose**: 1 (show training progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_categorical,\n",
    "    epochs=150,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val_scaled, y_val_categorical),\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Visualize Training Progress\n",
    "\n",
    "**Solution Explanation:**\n",
    "Training visualization helps us understand:\n",
    "- **Loss curves**: How well the model is learning (decreasing loss is good)\n",
    "- **Accuracy curves**: Classification performance over time\n",
    "- **Overfitting detection**: Gap between training and validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (Categorical Crossentropy)')\n",
    "axes[0].set_title('Model Loss During Training')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy\n",
    "axes[1].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Model Accuracy During Training')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Training analysis\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "\n",
    "print(\"Training Analysis:\")\n",
    "print(f\"Final training loss: {final_train_loss:.4f}\")\n",
    "print(f\"Final validation loss: {final_val_loss:.4f}\")\n",
    "print(f\"Final training accuracy: {final_train_acc:.4f}\")\n",
    "print(f\"Final validation accuracy: {final_val_acc:.4f}\")\n",
    "print(f\"\\nOverfitting indicators:\")\n",
    "print(f\"Loss gap: {((final_val_loss - final_train_loss) / final_train_loss * 100):.1f}% higher validation loss\")\n",
    "print(f\"Accuracy gap: {((final_train_acc - final_val_acc) / final_train_acc * 100):.1f}% lower validation accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Test the Model\n",
    "\n",
    "**Solution Explanation:**\n",
    "Model evaluation on unseen test data gives us the true performance:\n",
    "- **Test Accuracy**: Percentage of correct predictions\n",
    "- **Classification Report**: Precision, recall, and F1-score for each class\n",
    "- **Confusion Matrix**: Shows prediction vs actual class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test_categorical, verbose=0)\n",
    "print(f\"Test Results:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.1f}%)\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_proba = model.predict(X_test_scaled)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "print(f\"\\nPrediction Analysis:\")\n",
    "print(f\"Correctly classified: {np.sum(y_pred == y_test)} out of {len(y_test)}\")\n",
    "print(f\"Misclassified: {np.sum(y_pred != y_test)} out of {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=wine.target_names))\n",
    "\n",
    "# Show first 15 predictions with confidence scores\n",
    "print(\"\\nFirst 15 Predictions with Confidence:\")\n",
    "print(\"Predicted | Actual | Confidence | Correct\")\n",
    "print(\"-\" * 45)\n",
    "for i in range(min(15, len(y_test))):\n",
    "    pred_class = y_pred[i]\n",
    "    actual_class = y_test[i]\n",
    "    confidence = np.max(y_pred_proba[i])\n",
    "    correct = \"âœ“\" if pred_class == actual_class else \"âœ—\"\n",
    "    \n",
    "    pred_name = wine.target_names[pred_class][:8]\n",
    "    actual_name = wine.target_names[actual_class][:8]\n",
    "    \n",
    "    print(f\"{pred_name:>8} | {actual_name:>6} | {confidence:>8.3f} | {correct:>7}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9: Visualize Classification Results\n",
    "\n",
    "**Solution Explanation:**\n",
    "Result visualization helps us understand model performance:\n",
    "- **Confusion Matrix**: Shows which classes are confused with each other\n",
    "- **Classification Metrics**: Precision, recall, F1-score per class\n",
    "- **Prediction Confidence**: Distribution of prediction probabilities\n",
    "- **Feature Importance**: Which features contribute most to predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=wine.target_names, yticklabels=wine.target_names,\n",
    "            ax=axes[0, 0])\n",
    "axes[0, 0].set_xlabel('Predicted')\n",
    "axes[0, 0].set_ylabel('Actual')\n",
    "axes[0, 0].set_title('Confusion Matrix')\n",
    "\n",
    "# 2. Classification metrics comparison\n",
    "report = classification_report(y_test, y_pred, target_names=wine.target_names, output_dict=True)\n",
    "metrics = ['precision', 'recall', 'f1-score']\n",
    "x_pos = np.arange(len(wine.target_names))\n",
    "width = 0.25\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    values = [report[class_name][metric] for class_name in wine.target_names]\n",
    "    axes[0, 1].bar(x_pos + i * width, values, width, label=metric.capitalize())\n",
    "\n",
    "axes[0, 1].set_xlabel('Wine Class')\n",
    "axes[0, 1].set_ylabel('Score')\n",
    "axes[0, 1].set_title('Classification Metrics by Class')\n",
    "axes[0, 1].set_xticks(x_pos + width)\n",
    "axes[0, 1].set_xticklabels(wine.target_names)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Prediction confidence distribution\n",
    "confidence_scores = np.max(y_pred_proba, axis=1)\n",
    "axes[1, 0].hist(confidence_scores, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[1, 0].axvline(np.mean(confidence_scores), color='red', linestyle='--', \n",
    "                   label=f'Mean: {np.mean(confidence_scores):.3f}')\n",
    "axes[1, 0].set_xlabel('Prediction Confidence')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Distribution of Prediction Confidence')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Correct vs Incorrect predictions confidence\n",
    "correct_mask = y_pred == y_test\n",
    "correct_conf = confidence_scores[correct_mask]\n",
    "incorrect_conf = confidence_scores[~correct_mask]\n",
    "\n",
    "axes[1, 1].hist(correct_conf, bins=15, alpha=0.7, label='Correct', color='green')\n",
    "axes[1, 1].hist(incorrect_conf, bins=15, alpha=0.7, label='Incorrect', color='red')\n",
    "axes[1, 1].set_xlabel('Prediction Confidence')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Confidence: Correct vs Incorrect Predictions')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance summary\n",
    "print(f\"\\nModel Performance Summary:\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.3f} ({test_accuracy*100:.1f}%)\")\n",
    "print(f\"Average Confidence: {np.mean(confidence_scores):.3f}\")\n",
    "print(f\"Correct Predictions Confidence: {np.mean(correct_conf):.3f}\")\n",
    "if len(incorrect_conf) > 0:\n",
    "    print(f\"Incorrect Predictions Confidence: {np.mean(incorrect_conf):.3f}\")\n",
    "else:\n",
    "    print(\"No incorrect predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10: Analyze Model Predictions\n",
    "\n",
    "**Solution Explanation:**\n",
    "Let's analyze which samples the model finds most difficult to classify and examine the decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze difficult predictions (lowest confidence)\n",
    "confidence_indices = np.argsort(confidence_scores)\n",
    "print(\"Most Uncertain Predictions (Lowest Confidence):\")\n",
    "print(\"Index | Predicted | Actual | Confidence | Features\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i in range(min(10, len(confidence_indices))):\n",
    "    idx = confidence_indices[i]\n",
    "    pred_class = y_pred[idx]\n",
    "    actual_class = y_test[idx]\n",
    "    confidence = confidence_scores[idx]\n",
    "    \n",
    "    # Show top 3 feature values for this sample\n",
    "    sample_features = X_test_scaled[idx]\n",
    "    top_features_idx = np.argsort(np.abs(sample_features))[-3:]\n",
    "    \n",
    "    print(f\"{idx:>5} | {wine.target_names[pred_class]:>9} | {wine.target_names[actual_class]:>6} | {confidence:>8.3f} | \", end=\"\")\n",
    "    for j, feat_idx in enumerate(top_features_idx):\n",
    "        if j > 0:\n",
    "            print(\", \", end=\"\")\n",
    "        print(f\"{wine.feature_names[feat_idx][:8]}:{sample_features[feat_idx]:.2f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "# Analyze class-wise performance\n",
    "print(\"\\nClass-wise Performance Analysis:\")\n",
    "for i, class_name in enumerate(wine.target_names):\n",
    "    class_mask = y_test == i\n",
    "    class_accuracy = np.mean(y_pred[class_mask] == y_test[class_mask])\n",
    "    class_confidence = np.mean(confidence_scores[class_mask])\n",
    "    print(f\"{class_name}: Accuracy={class_accuracy:.3f}, Avg Confidence={class_confidence:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Challenge: Improved Model with Regularization\n",
    "\n",
    "**Solution Explanation:**\n",
    "Let's try to improve our model with:\n",
    "- **Dropout layers**: Prevent overfitting by randomly setting some neurons to zero\n",
    "- **Batch normalization**: Normalize inputs to each layer\n",
    "- **Early stopping**: Stop training when validation loss stops improving\n",
    "- **Learning rate scheduling**: Reduce learning rate when loss plateaus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building improved model with regularization...\")\n",
    "\n",
    "# Build improved model with regularization\n",
    "improved_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile with custom learning rate\n",
    "improved_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "improved_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up callbacks for improved training\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=20,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=10,\n",
    "    min_lr=0.00001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training improved model with callbacks...\")\n",
    "improved_history = improved_model.fit(\n",
    "    X_train_scaled, y_train_categorical,\n",
    "    epochs=200,\n",
    "    batch_size=16,\n",
    "    validation_data=(X_val_scaled, y_val_categorical),\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining stopped after {len(improved_history.history['loss'])} epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate improved model\n",
    "improved_test_loss, improved_test_accuracy = improved_model.evaluate(X_test_scaled, y_test_categorical, verbose=0)\n",
    "improved_y_pred_proba = improved_model.predict(X_test_scaled)\n",
    "improved_y_pred = np.argmax(improved_y_pred_proba, axis=1)\n",
    "\n",
    "print(f\"Model Comparison:\")\n",
    "print(f\"Original Model  - Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.1f}%)\")\n",
    "print(f\"Improved Model  - Test Accuracy: {improved_test_accuracy:.4f} ({improved_test_accuracy*100:.1f}%)\")\n",
    "print(f\"\\nImprovement: {((improved_test_accuracy - test_accuracy) / test_accuracy * 100):.1f}% increase in accuracy\")\n",
    "\n",
    "# Detailed comparison\n",
    "print(\"\\nDetailed Comparison:\")\n",
    "print(\"\\nOriginal Model Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=wine.target_names))\n",
    "\n",
    "print(\"\\nImproved Model Classification Report:\")\n",
    "print(classification_report(y_test, improved_y_pred, target_names=wine.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare training histories\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Training loss comparison\n",
    "axes[0, 0].plot(history.history['loss'], label='Original Model', linewidth=2)\n",
    "axes[0, 0].plot(improved_history.history['loss'], label='Improved Model', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Training Loss')\n",
    "axes[0, 0].set_title('Training Loss Comparison')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation loss comparison\n",
    "axes[0, 1].plot(history.history['val_loss'], label='Original Model', linewidth=2)\n",
    "axes[0, 1].plot(improved_history.history['val_loss'], label='Improved Model', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Validation Loss')\n",
    "axes[0, 1].set_title('Validation Loss Comparison')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Training accuracy comparison\n",
    "axes[1, 0].plot(history.history['accuracy'], label='Original Model', linewidth=2)\n",
    "axes[1, 0].plot(improved_history.history['accuracy'], label='Improved Model', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Training Accuracy')\n",
    "axes[1, 0].set_title('Training Accuracy Comparison')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Validation accuracy comparison\n",
    "axes[1, 1].plot(history.history['val_accuracy'], label='Original Model', linewidth=2)\n",
    "axes[1, 1].plot(improved_history.history['val_accuracy'], label='Improved Model', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Validation Accuracy')\n",
    "axes[1, 1].set_title('Validation Accuracy Comparison')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Questions - Test Your Understanding\n",
    "\n",
    "Try to answer these questions to check your understanding:\n",
    "\n",
    "### 1. What does the softmax activation function do in the output layer?\n",
    "\n",
    "**Answer:** The softmax function converts the raw output scores (logits) into probabilities that sum to 1.0. It ensures each output represents the probability of belonging to that class, making it perfect for multiclass classification.\n",
    "\n",
    "### 2. Why do we use categorical crossentropy instead of mean squared error for classification?\n",
    "\n",
    "**Answer:** Categorical crossentropy is designed for probability distributions and penalizes confident wrong predictions more heavily. MSE treats all errors equally, while crossentropy focuses on the probability of the correct class, making it more suitable for classification tasks.\n",
    "\n",
    "### 3. What does a confusion matrix tell us about our model's performance?\n",
    "\n",
    "**Answer:** A confusion matrix shows exactly which classes are being confused with each other. The diagonal elements represent correct predictions, while off-diagonal elements show misclassifications. It helps identify which classes are hardest to distinguish.\n",
    "\n",
    "### 4. What is the difference between precision and recall?\n",
    "\n",
    "**Answer:** \n",
    "- **Precision**: Of all positive predictions, how many were actually correct? (TP / (TP + FP))\n",
    "- **Recall**: Of all actual positives, how many did we correctly identify? (TP / (TP + FN))\n",
    "- High precision = few false positives; High recall = few false negatives\n",
    "\n",
    "### 5. How do dropout and batch normalization help improve model performance?\n",
    "\n",
    "**Answer:** \n",
    "- **Dropout**: Randomly sets neurons to zero during training, preventing overfitting by forcing the network to not rely on specific neurons\n",
    "- **Batch Normalization**: Normalizes inputs to each layer, stabilizing training and allowing higher learning rates\n",
    "- Both techniques act as regularization, improving generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Feature Importance Analysis\n",
    "\n",
    "**Solution Explanation:**\n",
    "Let's analyze which features contribute most to the model's predictions using a simple technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple feature importance analysis\n",
    "# We'll use the weights from the first layer as a proxy for feature importance\n",
    "first_layer_weights = improved_model.layers[0].get_weights()[0]  # Shape: (13, 128)\n",
    "\n",
    "# Calculate feature importance as the sum of absolute weights\n",
    "feature_importance = np.sum(np.abs(first_layer_weights), axis=1)\n",
    "\n",
    "# Create DataFrame for easier visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': wine.feature_names,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(len(importance_df)), importance_df['importance'])\n",
    "plt.yticks(range(len(importance_df)), importance_df['feature'])\n",
    "plt.xlabel('Feature Importance (Sum of Absolute Weights)')\n",
    "plt.title('Feature Importance Analysis')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 5 Most Important Features:\")\n",
    "for i, (_, row) in enumerate(importance_df.head().iterrows()):\n",
    "    print(f\"{i+1}. {row['feature']}: {row['importance']:.3f}\")\n",
    "\n",
    "print(\"\\nLeast Important Features:\")\n",
    "for i, (_, row) in enumerate(importance_df.tail(3).iterrows()):\n",
    "    print(f\"{row['feature']}: {row['importance']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "ðŸŽ¯ **Main Learning Points:**\n",
    "\n",
    "1. **Classification vs Regression**: Different output layers (softmax vs linear), loss functions (categorical crossentropy vs MSE), and metrics (accuracy vs MAE)\n",
    "\n",
    "2. **Data preparation is crucial**: Stratified splits maintain class distribution, normalization helps convergence, one-hot encoding enables multiclass classification\n",
    "\n",
    "3. **Evaluation is multifaceted**: Accuracy, precision, recall, F1-score, and confusion matrices provide different insights\n",
    "\n",
    "4. **Regularization prevents overfitting**: Dropout, batch normalization, and early stopping improve generalization\n",
    "\n",
    "5. **Visualization aids understanding**: Training curves, confusion matrices, and confidence distributions reveal model behavior\n",
    "\n",
    "6. **Class imbalance matters**: Class weights help handle uneven distributions\n",
    "\n",
    "7. **Model comparison guides improvement**: Systematic comparison helps identify better architectures and hyperparameters\n",
    "\n",
    "**ðŸŽ‰ Classification exercise completed successfully!** You now have a solid foundation in neural network classification using TensorFlow and Keras.\n",
    "\n",
    "**Next Steps:**\n",
    "- Try different datasets (Iris, Digits, etc.)\n",
    "- Experiment with different architectures\n",
    "- Explore advanced techniques (ensemble methods, transfer learning)\n",
    "- Practice with real-world datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
